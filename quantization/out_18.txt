Files already downloaded and verified
Files already downloaded and verified
Epoch: 00 Train Loss: 2.206 Train Acc: 0.292 Eval Loss: 1.728 Eval Acc: 0.381
Epoch: 01 Train Loss: 1.558 Train Acc: 0.428 Eval Loss: 1.378 Eval Acc: 0.500
Epoch: 02 Train Loss: 1.354 Train Acc: 0.507 Eval Loss: 1.249 Eval Acc: 0.548
Epoch: 03 Train Loss: 1.219 Train Acc: 0.564 Eval Loss: 1.148 Eval Acc: 0.586
Epoch: 04 Train Loss: 1.098 Train Acc: 0.609 Eval Loss: 0.987 Eval Acc: 0.650
Epoch: 05 Train Loss: 1.000 Train Acc: 0.645 Eval Loss: 1.001 Eval Acc: 0.657
Epoch: 06 Train Loss: 0.942 Train Acc: 0.667 Eval Loss: 0.917 Eval Acc: 0.686
Epoch: 07 Train Loss: 0.883 Train Acc: 0.689 Eval Loss: 0.855 Eval Acc: 0.704
Epoch: 08 Train Loss: 0.837 Train Acc: 0.707 Eval Loss: 0.811 Eval Acc: 0.720
Epoch: 09 Train Loss: 0.795 Train Acc: 0.723 Eval Loss: 0.843 Eval Acc: 0.712
Epoch: 10 Train Loss: 0.767 Train Acc: 0.733 Eval Loss: 0.833 Eval Acc: 0.726
Epoch: 11 Train Loss: 0.740 Train Acc: 0.739 Eval Loss: 0.733 Eval Acc: 0.746
Epoch: 12 Train Loss: 0.710 Train Acc: 0.754 Eval Loss: 0.718 Eval Acc: 0.751
Epoch: 13 Train Loss: 0.689 Train Acc: 0.760 Eval Loss: 0.716 Eval Acc: 0.753
Epoch: 14 Train Loss: 0.671 Train Acc: 0.767 Eval Loss: 0.697 Eval Acc: 0.761
Epoch: 15 Train Loss: 0.645 Train Acc: 0.776 Eval Loss: 0.679 Eval Acc: 0.767
Epoch: 16 Train Loss: 0.632 Train Acc: 0.780 Eval Loss: 0.662 Eval Acc: 0.779
Epoch: 17 Train Loss: 0.611 Train Acc: 0.787 Eval Loss: 0.712 Eval Acc: 0.760
Epoch: 18 Train Loss: 0.598 Train Acc: 0.793 Eval Loss: 0.687 Eval Acc: 0.772
Epoch: 19 Train Loss: 0.595 Train Acc: 0.792 Eval Loss: 0.718 Eval Acc: 0.762
Epoch: 20 Train Loss: 0.580 Train Acc: 0.799 Eval Loss: 0.635 Eval Acc: 0.781
Epoch: 21 Train Loss: 0.561 Train Acc: 0.806 Eval Loss: 0.729 Eval Acc: 0.756
Epoch: 22 Train Loss: 0.555 Train Acc: 0.809 Eval Loss: 0.651 Eval Acc: 0.780
Epoch: 23 Train Loss: 0.546 Train Acc: 0.812 Eval Loss: 0.624 Eval Acc: 0.787
Epoch: 24 Train Loss: 0.533 Train Acc: 0.815 Eval Loss: 0.622 Eval Acc: 0.790
Epoch: 25 Train Loss: 0.525 Train Acc: 0.819 Eval Loss: 0.607 Eval Acc: 0.796
Epoch: 26 Train Loss: 0.515 Train Acc: 0.820 Eval Loss: 0.660 Eval Acc: 0.779
Epoch: 27 Train Loss: 0.510 Train Acc: 0.823 Eval Loss: 0.625 Eval Acc: 0.797
Epoch: 28 Train Loss: 0.503 Train Acc: 0.825 Eval Loss: 0.598 Eval Acc: 0.800
Epoch: 29 Train Loss: 0.500 Train Acc: 0.828 Eval Loss: 0.580 Eval Acc: 0.801
Epoch: 30 Train Loss: 0.494 Train Acc: 0.829 Eval Loss: 0.621 Eval Acc: 0.793
Epoch: 31 Train Loss: 0.489 Train Acc: 0.830 Eval Loss: 0.578 Eval Acc: 0.809
Epoch: 32 Train Loss: 0.476 Train Acc: 0.834 Eval Loss: 0.618 Eval Acc: 0.799
Epoch: 33 Train Loss: 0.480 Train Acc: 0.833 Eval Loss: 0.567 Eval Acc: 0.810
Epoch: 34 Train Loss: 0.472 Train Acc: 0.836 Eval Loss: 0.682 Eval Acc: 0.779
Epoch: 35 Train Loss: 0.466 Train Acc: 0.837 Eval Loss: 0.605 Eval Acc: 0.800
Epoch: 36 Train Loss: 0.466 Train Acc: 0.839 Eval Loss: 0.593 Eval Acc: 0.805
Epoch: 37 Train Loss: 0.459 Train Acc: 0.840 Eval Loss: 0.572 Eval Acc: 0.808
Epoch: 38 Train Loss: 0.442 Train Acc: 0.846 Eval Loss: 0.641 Eval Acc: 0.794
Epoch: 39 Train Loss: 0.447 Train Acc: 0.843 Eval Loss: 0.593 Eval Acc: 0.804
Epoch: 40 Train Loss: 0.441 Train Acc: 0.847 Eval Loss: 0.570 Eval Acc: 0.812
Epoch: 41 Train Loss: 0.445 Train Acc: 0.845 Eval Loss: 0.602 Eval Acc: 0.806
Epoch: 42 Train Loss: 0.436 Train Acc: 0.848 Eval Loss: 0.544 Eval Acc: 0.817
Epoch: 43 Train Loss: 0.429 Train Acc: 0.849 Eval Loss: 0.602 Eval Acc: 0.803
Epoch: 44 Train Loss: 0.430 Train Acc: 0.848 Eval Loss: 0.576 Eval Acc: 0.808
Epoch: 45 Train Loss: 0.429 Train Acc: 0.849 Eval Loss: 0.585 Eval Acc: 0.809
Epoch: 46 Train Loss: 0.420 Train Acc: 0.853 Eval Loss: 0.604 Eval Acc: 0.806
Epoch: 47 Train Loss: 0.415 Train Acc: 0.854 Eval Loss: 0.590 Eval Acc: 0.808
Epoch: 48 Train Loss: 0.425 Train Acc: 0.852 Eval Loss: 0.560 Eval Acc: 0.817
Epoch: 49 Train Loss: 0.418 Train Acc: 0.853 Eval Loss: 0.577 Eval Acc: 0.811
Epoch: 50 Train Loss: 0.413 Train Acc: 0.856 Eval Loss: 0.583 Eval Acc: 0.807
Epoch: 51 Train Loss: 0.411 Train Acc: 0.856 Eval Loss: 0.540 Eval Acc: 0.820
Epoch: 52 Train Loss: 0.405 Train Acc: 0.860 Eval Loss: 0.658 Eval Acc: 0.787
Epoch: 53 Train Loss: 0.406 Train Acc: 0.859 Eval Loss: 0.547 Eval Acc: 0.817
Epoch: 54 Train Loss: 0.395 Train Acc: 0.861 Eval Loss: 0.572 Eval Acc: 0.816
Epoch: 55 Train Loss: 0.396 Train Acc: 0.863 Eval Loss: 0.609 Eval Acc: 0.811
Epoch: 56 Train Loss: 0.397 Train Acc: 0.860 Eval Loss: 0.554 Eval Acc: 0.820
Epoch: 57 Train Loss: 0.391 Train Acc: 0.863 Eval Loss: 0.590 Eval Acc: 0.806
Epoch: 58 Train Loss: 0.394 Train Acc: 0.863 Eval Loss: 0.575 Eval Acc: 0.811
Epoch: 59 Train Loss: 0.392 Train Acc: 0.862 Eval Loss: 0.567 Eval Acc: 0.817
Epoch: 60 Train Loss: 0.389 Train Acc: 0.864 Eval Loss: 0.554 Eval Acc: 0.823
Epoch: 61 Train Loss: 0.387 Train Acc: 0.866 Eval Loss: 0.553 Eval Acc: 0.823
Epoch: 62 Train Loss: 0.386 Train Acc: 0.863 Eval Loss: 0.575 Eval Acc: 0.816
Epoch: 63 Train Loss: 0.374 Train Acc: 0.868 Eval Loss: 0.671 Eval Acc: 0.792
Epoch: 64 Train Loss: 0.383 Train Acc: 0.866 Eval Loss: 0.629 Eval Acc: 0.805
Epoch: 65 Train Loss: 0.382 Train Acc: 0.865 Eval Loss: 0.573 Eval Acc: 0.814
Epoch: 66 Train Loss: 0.380 Train Acc: 0.867 Eval Loss: 0.555 Eval Acc: 0.822
Epoch: 67 Train Loss: 0.370 Train Acc: 0.870 Eval Loss: 0.561 Eval Acc: 0.815
Epoch: 68 Train Loss: 0.373 Train Acc: 0.870 Eval Loss: 0.552 Eval Acc: 0.820
Epoch: 69 Train Loss: 0.371 Train Acc: 0.871 Eval Loss: 0.593 Eval Acc: 0.809
Epoch: 70 Train Loss: 0.369 Train Acc: 0.871 Eval Loss: 0.554 Eval Acc: 0.822
Epoch: 71 Train Loss: 0.367 Train Acc: 0.873 Eval Loss: 0.557 Eval Acc: 0.819
Epoch: 72 Train Loss: 0.375 Train Acc: 0.868 Eval Loss: 0.557 Eval Acc: 0.825
Epoch: 73 Train Loss: 0.365 Train Acc: 0.873 Eval Loss: 0.540 Eval Acc: 0.824
Epoch: 74 Train Loss: 0.361 Train Acc: 0.873 Eval Loss: 0.585 Eval Acc: 0.811
Epoch: 75 Train Loss: 0.361 Train Acc: 0.873 Eval Loss: 0.564 Eval Acc: 0.824
Epoch: 76 Train Loss: 0.364 Train Acc: 0.874 Eval Loss: 0.558 Eval Acc: 0.820
Epoch: 77 Train Loss: 0.357 Train Acc: 0.875 Eval Loss: 0.539 Eval Acc: 0.826
Epoch: 78 Train Loss: 0.361 Train Acc: 0.873 Eval Loss: 0.601 Eval Acc: 0.808
Epoch: 79 Train Loss: 0.362 Train Acc: 0.874 Eval Loss: 0.603 Eval Acc: 0.811
Epoch: 80 Train Loss: 0.354 Train Acc: 0.876 Eval Loss: 0.586 Eval Acc: 0.817
Epoch: 81 Train Loss: 0.362 Train Acc: 0.873 Eval Loss: 0.526 Eval Acc: 0.833
Epoch: 82 Train Loss: 0.354 Train Acc: 0.876 Eval Loss: 0.579 Eval Acc: 0.814
Epoch: 83 Train Loss: 0.350 Train Acc: 0.879 Eval Loss: 0.568 Eval Acc: 0.818
Epoch: 84 Train Loss: 0.352 Train Acc: 0.878 Eval Loss: 0.551 Eval Acc: 0.822
Epoch: 85 Train Loss: 0.352 Train Acc: 0.877 Eval Loss: 0.580 Eval Acc: 0.813
Epoch: 86 Train Loss: 0.349 Train Acc: 0.878 Eval Loss: 0.575 Eval Acc: 0.818
Epoch: 87 Train Loss: 0.355 Train Acc: 0.877 Eval Loss: 0.543 Eval Acc: 0.824
Epoch: 88 Train Loss: 0.345 Train Acc: 0.879 Eval Loss: 0.558 Eval Acc: 0.823
Epoch: 89 Train Loss: 0.347 Train Acc: 0.877 Eval Loss: 0.534 Eval Acc: 0.826
Epoch: 90 Train Loss: 0.348 Train Acc: 0.878 Eval Loss: 0.552 Eval Acc: 0.827
Epoch: 91 Train Loss: 0.343 Train Acc: 0.881 Eval Loss: 0.535 Eval Acc: 0.830
Epoch: 92 Train Loss: 0.346 Train Acc: 0.880 Eval Loss: 0.602 Eval Acc: 0.816
Epoch: 93 Train Loss: 0.341 Train Acc: 0.881 Eval Loss: 0.539 Eval Acc: 0.828
Epoch: 94 Train Loss: 0.339 Train Acc: 0.882 Eval Loss: 0.653 Eval Acc: 0.804
Epoch: 95 Train Loss: 0.340 Train Acc: 0.881 Eval Loss: 0.582 Eval Acc: 0.821
Epoch: 96 Train Loss: 0.347 Train Acc: 0.879 Eval Loss: 0.587 Eval Acc: 0.815
Epoch: 97 Train Loss: 0.338 Train Acc: 0.882 Eval Loss: 0.577 Eval Acc: 0.816
Epoch: 98 Train Loss: 0.341 Train Acc: 0.880 Eval Loss: 0.567 Eval Acc: 0.823
Epoch: 99 Train Loss: 0.340 Train Acc: 0.882 Eval Loss: 0.545 Eval Acc: 0.822
Epoch: 100 Train Loss: 0.223 Train Acc: 0.922 Eval Loss: 0.453 Eval Acc: 0.857
Epoch: 101 Train Loss: 0.181 Train Acc: 0.937 Eval Loss: 0.456 Eval Acc: 0.863
Epoch: 102 Train Loss: 0.171 Train Acc: 0.940 Eval Loss: 0.460 Eval Acc: 0.863
Epoch: 103 Train Loss: 0.157 Train Acc: 0.945 Eval Loss: 0.464 Eval Acc: 0.865
Epoch: 104 Train Loss: 0.149 Train Acc: 0.948 Eval Loss: 0.480 Eval Acc: 0.864
Epoch: 105 Train Loss: 0.140 Train Acc: 0.951 Eval Loss: 0.475 Eval Acc: 0.866
Epoch: 106 Train Loss: 0.135 Train Acc: 0.952 Eval Loss: 0.480 Eval Acc: 0.865
Epoch: 107 Train Loss: 0.129 Train Acc: 0.955 Eval Loss: 0.482 Eval Acc: 0.867
Epoch: 108 Train Loss: 0.122 Train Acc: 0.957 Eval Loss: 0.489 Eval Acc: 0.868
Epoch: 109 Train Loss: 0.118 Train Acc: 0.959 Eval Loss: 0.489 Eval Acc: 0.869
Epoch: 110 Train Loss: 0.115 Train Acc: 0.960 Eval Loss: 0.500 Eval Acc: 0.867
Epoch: 111 Train Loss: 0.111 Train Acc: 0.961 Eval Loss: 0.510 Eval Acc: 0.865
Epoch: 112 Train Loss: 0.107 Train Acc: 0.962 Eval Loss: 0.514 Eval Acc: 0.867
Epoch: 113 Train Loss: 0.104 Train Acc: 0.963 Eval Loss: 0.505 Eval Acc: 0.869
Epoch: 114 Train Loss: 0.104 Train Acc: 0.964 Eval Loss: 0.510 Eval Acc: 0.868
Epoch: 115 Train Loss: 0.099 Train Acc: 0.965 Eval Loss: 0.520 Eval Acc: 0.865
Epoch: 116 Train Loss: 0.098 Train Acc: 0.966 Eval Loss: 0.522 Eval Acc: 0.868
Epoch: 117 Train Loss: 0.096 Train Acc: 0.965 Eval Loss: 0.519 Eval Acc: 0.868
Epoch: 118 Train Loss: 0.093 Train Acc: 0.968 Eval Loss: 0.536 Eval Acc: 0.866
Epoch: 119 Train Loss: 0.088 Train Acc: 0.969 Eval Loss: 0.541 Eval Acc: 0.867
Epoch: 120 Train Loss: 0.090 Train Acc: 0.969 Eval Loss: 0.537 Eval Acc: 0.867
Epoch: 121 Train Loss: 0.087 Train Acc: 0.970 Eval Loss: 0.542 Eval Acc: 0.867
Epoch: 122 Train Loss: 0.086 Train Acc: 0.970 Eval Loss: 0.550 Eval Acc: 0.866
Epoch: 123 Train Loss: 0.084 Train Acc: 0.970 Eval Loss: 0.554 Eval Acc: 0.867
Epoch: 124 Train Loss: 0.081 Train Acc: 0.972 Eval Loss: 0.548 Eval Acc: 0.870
Epoch: 125 Train Loss: 0.078 Train Acc: 0.972 Eval Loss: 0.554 Eval Acc: 0.869
Epoch: 126 Train Loss: 0.078 Train Acc: 0.973 Eval Loss: 0.555 Eval Acc: 0.867
Epoch: 127 Train Loss: 0.076 Train Acc: 0.973 Eval Loss: 0.567 Eval Acc: 0.867
Epoch: 128 Train Loss: 0.072 Train Acc: 0.975 Eval Loss: 0.568 Eval Acc: 0.865
Epoch: 129 Train Loss: 0.073 Train Acc: 0.975 Eval Loss: 0.581 Eval Acc: 0.863
Epoch: 130 Train Loss: 0.071 Train Acc: 0.975 Eval Loss: 0.589 Eval Acc: 0.866
Epoch: 131 Train Loss: 0.072 Train Acc: 0.975 Eval Loss: 0.590 Eval Acc: 0.865
Epoch: 132 Train Loss: 0.071 Train Acc: 0.975 Eval Loss: 0.587 Eval Acc: 0.865
Epoch: 133 Train Loss: 0.069 Train Acc: 0.975 Eval Loss: 0.578 Eval Acc: 0.867
Epoch: 134 Train Loss: 0.066 Train Acc: 0.977 Eval Loss: 0.589 Eval Acc: 0.866
Epoch: 135 Train Loss: 0.067 Train Acc: 0.976 Eval Loss: 0.590 Eval Acc: 0.868
Epoch: 136 Train Loss: 0.065 Train Acc: 0.977 Eval Loss: 0.608 Eval Acc: 0.863
Epoch: 137 Train Loss: 0.064 Train Acc: 0.977 Eval Loss: 0.595 Eval Acc: 0.866
Epoch: 138 Train Loss: 0.060 Train Acc: 0.979 Eval Loss: 0.600 Eval Acc: 0.866
Epoch: 139 Train Loss: 0.064 Train Acc: 0.978 Eval Loss: 0.605 Eval Acc: 0.869
Epoch: 140 Train Loss: 0.061 Train Acc: 0.978 Eval Loss: 0.611 Eval Acc: 0.865
Epoch: 141 Train Loss: 0.060 Train Acc: 0.979 Eval Loss: 0.614 Eval Acc: 0.866
Epoch: 142 Train Loss: 0.060 Train Acc: 0.979 Eval Loss: 0.607 Eval Acc: 0.864
Epoch: 143 Train Loss: 0.058 Train Acc: 0.979 Eval Loss: 0.603 Eval Acc: 0.866
Epoch: 144 Train Loss: 0.057 Train Acc: 0.979 Eval Loss: 0.625 Eval Acc: 0.866
Epoch: 145 Train Loss: 0.057 Train Acc: 0.980 Eval Loss: 0.633 Eval Acc: 0.864
Epoch: 146 Train Loss: 0.058 Train Acc: 0.979 Eval Loss: 0.632 Eval Acc: 0.863
Epoch: 147 Train Loss: 0.054 Train Acc: 0.981 Eval Loss: 0.625 Eval Acc: 0.866
Epoch: 148 Train Loss: 0.060 Train Acc: 0.979 Eval Loss: 0.627 Eval Acc: 0.865
Epoch: 149 Train Loss: 0.055 Train Acc: 0.980 Eval Loss: 0.627 Eval Acc: 0.867
Epoch: 150 Train Loss: 0.047 Train Acc: 0.984 Eval Loss: 0.613 Eval Acc: 0.870
Epoch: 151 Train Loss: 0.043 Train Acc: 0.985 Eval Loss: 0.614 Eval Acc: 0.869
Epoch: 152 Train Loss: 0.040 Train Acc: 0.986 Eval Loss: 0.617 Eval Acc: 0.870
Epoch: 153 Train Loss: 0.038 Train Acc: 0.987 Eval Loss: 0.612 Eval Acc: 0.870
Epoch: 154 Train Loss: 0.040 Train Acc: 0.987 Eval Loss: 0.610 Eval Acc: 0.870
Epoch: 155 Train Loss: 0.036 Train Acc: 0.988 Eval Loss: 0.618 Eval Acc: 0.871
Epoch: 156 Train Loss: 0.035 Train Acc: 0.988 Eval Loss: 0.625 Eval Acc: 0.871
Epoch: 157 Train Loss: 0.035 Train Acc: 0.989 Eval Loss: 0.618 Eval Acc: 0.871
Epoch: 158 Train Loss: 0.035 Train Acc: 0.988 Eval Loss: 0.625 Eval Acc: 0.871
Epoch: 159 Train Loss: 0.033 Train Acc: 0.989 Eval Loss: 0.629 Eval Acc: 0.872
Epoch: 160 Train Loss: 0.034 Train Acc: 0.989 Eval Loss: 0.624 Eval Acc: 0.871
Epoch: 161 Train Loss: 0.034 Train Acc: 0.989 Eval Loss: 0.629 Eval Acc: 0.870
Epoch: 162 Train Loss: 0.032 Train Acc: 0.989 Eval Loss: 0.627 Eval Acc: 0.871
Epoch: 163 Train Loss: 0.032 Train Acc: 0.989 Eval Loss: 0.633 Eval Acc: 0.870
Epoch: 164 Train Loss: 0.031 Train Acc: 0.989 Eval Loss: 0.632 Eval Acc: 0.871
Epoch: 165 Train Loss: 0.031 Train Acc: 0.990 Eval Loss: 0.632 Eval Acc: 0.871
Epoch: 166 Train Loss: 0.030 Train Acc: 0.990 Eval Loss: 0.633 Eval Acc: 0.871
Epoch: 167 Train Loss: 0.033 Train Acc: 0.989 Eval Loss: 0.633 Eval Acc: 0.871
Epoch: 168 Train Loss: 0.029 Train Acc: 0.990 Eval Loss: 0.642 Eval Acc: 0.872
Epoch: 169 Train Loss: 0.030 Train Acc: 0.990 Eval Loss: 0.639 Eval Acc: 0.872
Epoch: 170 Train Loss: 0.029 Train Acc: 0.990 Eval Loss: 0.640 Eval Acc: 0.873
Epoch: 171 Train Loss: 0.029 Train Acc: 0.991 Eval Loss: 0.640 Eval Acc: 0.870
Epoch: 172 Train Loss: 0.029 Train Acc: 0.991 Eval Loss: 0.639 Eval Acc: 0.873
Epoch: 173 Train Loss: 0.026 Train Acc: 0.992 Eval Loss: 0.646 Eval Acc: 0.871
Epoch: 174 Train Loss: 0.028 Train Acc: 0.990 Eval Loss: 0.647 Eval Acc: 0.871
Epoch: 175 Train Loss: 0.029 Train Acc: 0.990 Eval Loss: 0.647 Eval Acc: 0.872
Epoch: 176 Train Loss: 0.029 Train Acc: 0.990 Eval Loss: 0.643 Eval Acc: 0.872
Epoch: 177 Train Loss: 0.025 Train Acc: 0.992 Eval Loss: 0.643 Eval Acc: 0.872
Epoch: 178 Train Loss: 0.027 Train Acc: 0.991 Eval Loss: 0.648 Eval Acc: 0.871
Epoch: 179 Train Loss: 0.027 Train Acc: 0.991 Eval Loss: 0.651 Eval Acc: 0.871
Epoch: 180 Train Loss: 0.027 Train Acc: 0.991 Eval Loss: 0.658 Eval Acc: 0.872
Epoch: 181 Train Loss: 0.026 Train Acc: 0.992 Eval Loss: 0.654 Eval Acc: 0.872
Epoch: 182 Train Loss: 0.027 Train Acc: 0.991 Eval Loss: 0.653 Eval Acc: 0.872
Epoch: 183 Train Loss: 0.026 Train Acc: 0.991 Eval Loss: 0.661 Eval Acc: 0.872
Epoch: 184 Train Loss: 0.026 Train Acc: 0.992 Eval Loss: 0.658 Eval Acc: 0.871
Epoch: 185 Train Loss: 0.026 Train Acc: 0.991 Eval Loss: 0.662 Eval Acc: 0.870
Epoch: 186 Train Loss: 0.026 Train Acc: 0.991 Eval Loss: 0.660 Eval Acc: 0.871
Epoch: 187 Train Loss: 0.025 Train Acc: 0.992 Eval Loss: 0.662 Eval Acc: 0.869
Epoch: 188 Train Loss: 0.025 Train Acc: 0.992 Eval Loss: 0.662 Eval Acc: 0.872
Epoch: 189 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.665 Eval Acc: 0.871
Epoch: 190 Train Loss: 0.025 Train Acc: 0.992 Eval Loss: 0.671 Eval Acc: 0.870
Epoch: 191 Train Loss: 0.025 Train Acc: 0.992 Eval Loss: 0.665 Eval Acc: 0.870
Epoch: 192 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.670 Eval Acc: 0.872
Epoch: 193 Train Loss: 0.026 Train Acc: 0.991 Eval Loss: 0.667 Eval Acc: 0.869
Epoch: 194 Train Loss: 0.023 Train Acc: 0.992 Eval Loss: 0.666 Eval Acc: 0.871
Epoch: 195 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.675 Eval Acc: 0.871
Epoch: 196 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.675 Eval Acc: 0.872
Epoch: 197 Train Loss: 0.023 Train Acc: 0.993 Eval Loss: 0.680 Eval Acc: 0.870
Epoch: 198 Train Loss: 0.023 Train Acc: 0.993 Eval Loss: 0.676 Eval Acc: 0.871
Epoch: 199 Train Loss: 0.023 Train Acc: 0.992 Eval Loss: 0.677 Eval Acc: 0.871
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (relu2): ReLU(inplace=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (relu2): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (relu2): ReLU(inplace=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (relu2): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (relu2): ReLU(inplace=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (relu2): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (relu2): ReLU(inplace=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (relu2): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
ResNet(
  (conv1): ConvReLU2d(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))
    (1): ReLU(inplace=True)
  )
  (bn1): Identity()
  (relu): Identity()
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): ConvReLU2d(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
      (bn1): Identity()
      (relu1): Identity()
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn2): Identity()
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (relu2): ReLU(inplace=True)
    )
    (1): BasicBlock(
      (conv1): ConvReLU2d(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
      (bn1): Identity()
      (relu1): Identity()
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn2): Identity()
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (relu2): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): ConvReLU2d(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
      (bn1): Identity()
      (relu1): Identity()
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn2): Identity()
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))
        (1): Identity()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (relu2): ReLU(inplace=True)
    )
    (1): BasicBlock(
      (conv1): ConvReLU2d(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
      (bn1): Identity()
      (relu1): Identity()
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn2): Identity()
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (relu2): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): ConvReLU2d(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
      (bn1): Identity()
      (relu1): Identity()
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn2): Identity()
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))
        (1): Identity()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (relu2): ReLU(inplace=True)
    )
    (1): BasicBlock(
      (conv1): ConvReLU2d(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
      (bn1): Identity()
      (relu1): Identity()
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn2): Identity()
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (relu2): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): ConvReLU2d(
        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
      (bn1): Identity()
      (relu1): Identity()
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn2): Identity()
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))
        (1): Identity()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (relu2): ReLU(inplace=True)
    )
    (1): BasicBlock(
      (conv1): ConvReLU2d(
        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
      (bn1): Identity()
      (relu1): Identity()
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn2): Identity()
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (relu2): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:174: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.
  reduce_range will be deprecated in a future release of PyTorch."
/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:886: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  src_bin_begin // dst_bin_width, 0, self.dst_nbins - 1
/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:891: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  src_bin_end // dst_bin_width, 0, self.dst_nbins - 1
QuantizedResNet18(
  (quant): Quantize(scale=tensor([0.0374]), zero_point=tensor([57]), dtype=torch.quint8)
  (dequant): DeQuantize()
  (model_fp32): ResNet(
    (conv1): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.054884932935237885, zero_point=0, padding=(3, 3))
    (bn1): Identity()
    (relu): Identity()
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.026811648160219193, zero_point=0, padding=(1, 1))
        (bn1): Identity()
        (relu1): Identity()
        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0738033652305603, zero_point=75, padding=(1, 1))
        (bn2): Identity()
        (skip_add): QFunctional(
          scale=0.1130937710404396, zero_point=41
          (activation_post_process): Identity()
        )
        (relu2): ReLU(inplace=True)
      )
      (1): BasicBlock(
        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.03308633342385292, zero_point=0, padding=(1, 1))
        (bn1): Identity()
        (relu1): Identity()
        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.07694347947835922, zero_point=74, padding=(1, 1))
        (bn2): Identity()
        (skip_add): QFunctional(
          scale=0.10235446691513062, zero_point=55
          (activation_post_process): Identity()
        )
        (relu2): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.027488339692354202, zero_point=0, padding=(1, 1))
        (bn1): Identity()
        (relu1): Identity()
        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.08721021562814713, zero_point=46, padding=(1, 1))
        (bn2): Identity()
        (downsample): Sequential(
          (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.044375304132699966, zero_point=61)
          (1): Identity()
        )
        (skip_add): QFunctional(
          scale=0.0739443227648735, zero_point=63
          (activation_post_process): Identity()
        )
        (relu2): ReLU(inplace=True)
      )
      (1): BasicBlock(
        (conv1): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.024230895563960075, zero_point=0, padding=(1, 1))
        (bn1): Identity()
        (relu1): Identity()
        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.055538058280944824, zero_point=75, padding=(1, 1))
        (bn2): Identity()
        (skip_add): QFunctional(
          scale=0.07746735215187073, zero_point=53
          (activation_post_process): Identity()
        )
        (relu2): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.020770594477653503, zero_point=0, padding=(1, 1))
        (bn1): Identity()
        (relu1): Identity()
        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.04114682599902153, zero_point=69, padding=(1, 1))
        (bn2): Identity()
        (downsample): Sequential(
          (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.0268500205129385, zero_point=72)
          (1): Identity()
        )
        (skip_add): QFunctional(
          scale=0.04214794561266899, zero_point=74
          (activation_post_process): Identity()
        )
        (relu2): ReLU(inplace=True)
      )
      (1): BasicBlock(
        (conv1): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.005799301899969578, zero_point=0, padding=(1, 1))
        (bn1): Identity()
        (relu1): Identity()
        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.0252222940325737, zero_point=77, padding=(1, 1))
        (bn2): Identity()
        (skip_add): QFunctional(
          scale=0.04006292670965195, zero_point=56
          (activation_post_process): Identity()
        )
        (relu2): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.010442095808684826, zero_point=0, padding=(1, 1))
        (bn1): Identity()
        (relu1): Identity()
        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.02527034655213356, zero_point=65, padding=(1, 1))
        (bn2): Identity()
        (downsample): Sequential(
          (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.03231005370616913, zero_point=58)
          (1): Identity()
        )
        (skip_add): QFunctional(
          scale=0.04374627023935318, zero_point=48
          (activation_post_process): Identity()
        )
        (relu2): ReLU(inplace=True)
      )
      (1): BasicBlock(
        (conv1): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.001499424921348691, zero_point=0, padding=(1, 1))
        (bn1): Identity()
        (relu1): Identity()
        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.010569771751761436, zero_point=77, padding=(1, 1))
        (bn2): Identity()
        (skip_add): QFunctional(
          scale=0.03217065706849098, zero_point=25
          (activation_post_process): Identity()
        )
        (relu2): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): QuantizedLinear(in_features=512, out_features=10, scale=0.2568211853504181, zero_point=47, qscheme=torch.per_channel_affine)
  )
)
/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:886: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  src_bin_begin // dst_bin_width, 0, self.dst_nbins - 1
/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:891: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  src_bin_end // dst_bin_width, 0, self.dst_nbins - 1
FP32 evaluation accuracy: 0.8709
INT8 evaluation accuracy: 0.8673
FP32 CPU Inference Latency: 8.44 ms / sample
INT8 CPU Inference Latency: 4.79 ms / sample
INT8 JIT CPU Inference Latency: 3.11 ms / sample
    

