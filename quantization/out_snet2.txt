Files already downloaded and verified
Files already downloaded and verified
Epoch: 00 Train Loss: 2.164 Train Acc: 0.175 Eval Loss: 2.153 Eval Acc: 0.199
Epoch: 01 Train Loss: 2.125 Train Acc: 0.196 Eval Loss: 2.086 Eval Acc: 0.213
SNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))
  (pool1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))
  (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))
  (conv4): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))
  (fc1): Linear(in_features=512, out_features=10, bias=True)
  (relu1): ReLU(inplace=True)
  (relu2): ReLU(inplace=True)
  (relu3): ReLU(inplace=True)
  (relu4): ReLU(inplace=True)
)
SNet(
  (conv1): ConvReLU2d(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))
    (1): ReLU(inplace=True)
  )
  (pool1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (conv2): ConvReLU2d(
    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))
    (1): ReLU(inplace=True)
  )
  (conv3): ConvReLU2d(
    (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))
    (1): ReLU(inplace=True)
  )
  (conv4): ConvReLU2d(
    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))
    (1): ReLU(inplace=True)
  )
  (fc1): Linear(in_features=512, out_features=10, bias=True)
  (relu1): Identity()
  (relu2): Identity()
  (relu3): Identity()
  (relu4): Identity()
)
QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:174: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.
  reduce_range will be deprecated in a future release of PyTorch."
QuantizedResNet18(
  (quant): Quantize(scale=tensor([0.0374]), zero_point=tensor([57]), dtype=torch.quint8)
  (dequant): DeQuantize()
  (model_fp32): SNet(
    (conv1): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.10189351439476013, zero_point=0, padding=(3, 3))
    (pool1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (conv2): QuantizedConvReLU2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.06353111565113068, zero_point=0)
    (conv3): QuantizedConvReLU2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.026051361113786697, zero_point=0)
    (conv4): QuantizedConvReLU2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.011950025334954262, zero_point=0)
    (fc1): QuantizedLinear(in_features=512, out_features=10, scale=0.05280588939785957, zero_point=61, qscheme=torch.per_channel_affine)
    (relu1): Identity()
    (relu2): Identity()
    (relu3): Identity()
    (relu4): Identity()
  )
)
/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:886: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  src_bin_begin // dst_bin_width, 0, self.dst_nbins - 1
/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:891: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  src_bin_end // dst_bin_width, 0, self.dst_nbins - 1
FP32 evaluation accuracy: 0.2132
INT8 evaluation accuracy: 0.2145
FP32 CPU Inference Latency: 3.03 ms / sample
INT8 CPU Inference Latency: 0.76 ms / sample
INT8 JIT CPU Inference Latency: 0.59 ms / sample
